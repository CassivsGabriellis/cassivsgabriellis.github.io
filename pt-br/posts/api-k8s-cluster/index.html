<!doctype html><html lang=pt-br dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><link rel=icon type=image/ico href=https://www.foliocassianico.com.br/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.foliocassianico.com.br/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.foliocassianico.com.br/favicon-32x32.png><link rel=icon type=image/png sizes=192x192 href=https://www.foliocassianico.com.br/android-chrome-192x192.png><link rel=apple-touch-icon sizes=180x180 href=https://www.foliocassianico.com.br/apple-touch-icon.png><meta name=description content><title>Construindo uma API de m√©tricas em AWS EC2 com FastAPI, k3s (Kubernetes), Terraform, Ansible e CI/CD com GitHub Actions | C√°ssio Gabriel</title><link rel=canonical href=https://www.foliocassianico.com.br/pt-br/posts/api-k8s-cluster/><meta property="og:url" content="https://www.foliocassianico.com.br/pt-br/posts/api-k8s-cluster/"><meta property="og:site_name" content="C√°ssio Gabriel"><meta property="og:title" content="Construindo uma API de m√©tricas em AWS EC2 com FastAPI, k3s (Kubernetes), Terraform, Ansible e CI/CD com GitHub Actions"><meta property="og:description" content="API em FastAPI executando em um cluster k3s dentro de uma inst√¢ncia AWS EC2, expondo m√©tricas de performance da m√°quina e da aplica√ß√£o, provisionada com Terraform, configurada com Ansible e atualizada via GitHub Actions."><meta property="og:locale" content="pt_br"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-15T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-22T18:40:34+00:00"><meta property="article:tag" content="Cloud"><meta property="article:tag" content="Devops"><meta property="article:tag" content="System-Design"><meta property="article:tag" content="Aws"><meta property="article:tag" content="Ec2"><meta property="article:tag" content="Kubernetes"><link rel=stylesheet href=/assets/combined.min.1d9ba38136d24bf5ecdbf4eb600787b05e3af31c151dc3b703610e694f16b88f.css media=all></head><body class=auto><div class=content><header><div class=header><div class=language-switcher><a href=/en/posts/api-k8s-cluster/ class=lang-link>EN
</a><a href=/pt-br/posts/api-k8s-cluster/ class="lang-link active-lang">PT(BR)</a></div><style>.header{position:relative}.language-switcher{position:absolute;top:3.5rem;left:40%;transform:translateX(250px);z-index:50;display:flex;gap:.4rem;font-size:.9rem}.lang-link{text-decoration:none;color:var(--primary);opacity:.7}.lang-link.active-lang{font-weight:700;opacity:1}@media(max-width:700px){.language-switcher{top:.4rem;left:auto;right:1rem;transform:none;font-size:.85rem}}</style><button id=theme-toggle class=theme-toggle type=button aria-label="Switch between dark and bright themes">
<span class="theme-toggle__icon sun">‚òÄÔ∏è</span>
<span class="theme-toggle__icon moon">üåô</span>
</button>
<script>(function(){const n="typo-color-mode",s=document.getElementById("theme-toggle");if(!s)return;function e(){return document.body.classList.contains("dark")}function t(){const t=document.documentElement;t.setAttribute("data-theme",e()?"dark":"light")}function o(){const o=localStorage.getItem(n);if(!o){t();return}const s=o==="dark";typeof invertBody=="function"&&s!==e()?invertBody():typeof invertBody!="function"&&(document.body.classList.toggle("dark",s),document.body.classList.toggle("light",!s)),t()}s.addEventListener("click",function(){typeof invertBody=="function"?invertBody():(document.body.classList.toggle("dark"),document.body.classList.toggle("light"));const s=e()?"dark":"light";localStorage.setItem(n,s),t()}),document.addEventListener("DOMContentLoaded",o)})()</script><style>.theme-toggle{position:absolute;top:3.5rem;left:27%;transform:translateX(300px);z-index:50;background:0 0;border:none;cursor:pointer;display:flex;align-items:center;gap:.3rem;font-size:1.1rem;opacity:.8;transition:opacity .2s ease}.theme-toggle:hover{opacity:.5}.theme-toggle__icon{font-size:1.2rem;line-height:1}.theme-toggle__icon.sun,.theme-toggle__icon.moon{display:none}html[data-theme=light] .theme-toggle__icon.moon{display:inline}html[data-theme=dark] .theme-toggle__icon.sun{display:inline}@media(max-width:700px){.theme-toggle{top:.4rem;left:auto;right:6rem;transform:none;font-size:1.1rem}}</style><h1 class=header-title>C√°ssio Gabriel</h1><div class=header-menu><p class=small><a href=/pt-br/>/in√≠cio</a></p><p class=small><a href=/pt-br/projects>/projetos</a></p><p class=small><a href=/pt-br/posts>/posts</a></p></div></div></header><main class=main><div class=breadcrumbs><a href=/pt-br/>~</a><span class=breadcrumbs-separator>/</span><a href=/pt-br/posts/>Posts</a><span class=breadcrumbs-separator>/</span>
<a href=/pt-br/posts/api-k8s-cluster/>Construindo uma API de m√©tricas em AWS EC2 com FastAPI, k3s (Kubernetes), Terraform, Ansible e CI/CD com GitHub Actions</a></div><div class=autonumber><article><header class=single-intro-container><h1 class=single-title>Construindo uma API de m√©tricas em AWS EC2 com FastAPI, k3s (Kubernetes), Terraform, Ansible e CI/CD com GitHub Actions</h1><p class=single-summary>API em FastAPI executando em um cluster k3s dentro de uma inst√¢ncia AWS EC2, expondo m√©tricas de performance da m√°quina e da aplica√ß√£o, provisionada com Terraform, configurada com Ansible e atualizada via GitHub Actions.</p><div class=single-subsummary><div><p class=single-date>Publicado em:
<time datetime=2025-11-15T00:00:00+00:00>15 de novembro de 2025</time>
&nbsp; ¬∑ &nbsp;
√öltima atualiza√ß√£o:
<time datetime=2025-11-22T18:40:34+00:00>22 de novembro de 2025</time>
&nbsp; ¬∑ &nbsp;21 min de leitura</p></div></div></header><div class=single-layout><div class=single-main><div class=single-content><h2 class=heading id=propr√≥sito-deste-projeto>Propr√≥sito deste projeto
<a class=anchor href=#propr%c3%b3sito-deste-projeto>#</a></h2><p>Em busca de aprofundar meus conhecimentos em arquitetura de nuvem e nas variadas formas de se construir ambientes em nuvem, decidi construir do zero uma API e provision√°-la em uma inst√¢ncia EC2 Linux para testar certos conceitos dos quais tenho aprendido nos √∫ltimos dias, este principalmente em meus estudos para certifica√ß√£o AWS Cloud Architect.</p><p>Objetivamente, desenvolvi uma API em Python (FastAPI) que coleta m√©tricas da inst√¢ncia e da aplica√ß√£o nela sendo executada e a coloquei para rodar em um cluster Kubernetes numa inst√¢ncia EC2 na AWS. Toda a infraestrutura √© provisionada com Terraform, a configura√ß√£o do servidor e do cluster √© feita com Ansible, e o deploy √© automatizado com GitHub Actions. O projeto foi pensado para ser integrado posteriormente com Prometheus e Grafana para observabilidade completa.</p><h3 class=heading id=objetivos-deste-projeto>Objetivos deste projeto
<a class=anchor href=#objetivos-deste-projeto>#</a></h3><ul><li>Construir uma API simples que expor√° m√©tricas de performance da inst√¢ncia e da pr√≥pria aplica√ß√£o, em Python (servida pelo web framework FastAPI);</li><li>Rodar√° num cluster Kubernetes dentro de uma inst√¢ncia AWS EC2 Linux;</li><li>Ser√° provisionada por Terraform (VPC, sub-rede, EC2, Security Groups, IAM);</li><li>Configurada com Ansible (instala Docker, K3s, depend√™ncias, faz deploy);</li><li>Atualizada via CI/CD com GitHub Actions (build da imagem e pull do DockerHub, push, deploy autom√°tico).</li></ul><h3 class=heading id=diagrama-de-arquitetura-do-projeto>Diagrama de arquitetura do projeto
<a class=anchor href=#diagrama-de-arquitetura-do-projeto>#</a></h3><p><figure><div class=img-container style=--w:1669;--h:1813><img loading=lazy alt src=/pt-br/posts/api-k8s-cluster/images/metrics-api-k3s-ec2-cicd-architecture.png width=1669 height=1813></div><div class=caption-container><figcaption>Diagrama feito com a biblioteca Diagrams (Diagram as Code)</figcaption></div></figure></p><hr><h2 class=heading id=api-com-fastapi>API com FastAPI
<a class=anchor href=#api-com-fastapi>#</a></h2><p>O papel desta API √© expor m√©tricas de desempenho tanto do host onde o pod est√° executando quanto da pr√≥pria aplica√ß√£o, isto √©, a API em si, software backend escrito em Python, empacotado em Docker e executado dentro de um cluster Kubernetes ‚Äî cujo prop√≥sito √© coletar e expor m√©tricas operacionais tanto do pr√≥prio ambiente onde est√° rodando quanto do seu estado interno.</p><p>Trata-se de uma camada fundamental para permitir integra√ß√£o futura com ferramentas de observabilidade como Prometheus e Grafana. Para isso, optei pelo framework <a href=https://fastapi.tiangolo.com/>FastAPI</a>, que oferece rotas r√°pidas, excelente performance ass√≠ncrona e um modelo interno de documenta√ß√£o autom√°tica. Essa escolha viabiliza uma comunica√ß√£o eficiente entre os componentes distribu√≠dos do cluster, al√©m de proporcionar simplicidade na implementa√ß√£o dos endpoints.</p><p>O design da API serve os seguintes <em>endpoints</em>:</p><ul><li><code>GET /health</code>: usado pelo Kubernetes para testes de Liveness e Readiness probe.<ul><li>Exemplo:</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;status&#34;</span><span class=p>:</span> <span class=s2>&#34;ok&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;app&#34;</span><span class=p>:</span> <span class=s2>&#34;k8s-cluster-performance-stack&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;version&#34;</span><span class=p>:</span> <span class=s2>&#34;1.0.0&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></li><li><code>GET /info</code>: fornece informa√ß√µes sobre a aplica√ß√£o que est√° rodando na inst√¢ncia.<ul><li>Exemplo:</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;app_name&#34;</span><span class=p>:</span> <span class=s2>&#34;k8s-cluster-performance-stack&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;version&#34;</span><span class=p>:</span> <span class=s2>&#34;1.0.0&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;environment&#34;</span><span class=p>:</span> <span class=s2>&#34;dev&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;server_time&#34;</span><span class=p>:</span> <span class=s2>&#34;2025-11-15T15:30:00Z&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></li><li><code>GET /metrics/system</code>: m√©tricas do host onde o <em>pod</em> est√° rodando.<ul><li>Exemplo:</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;cpu&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;percent&#34;</span><span class=p>:</span> <span class=mf>21.3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;cores&#34;</span><span class=p>:</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;memory&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;total_mb&#34;</span><span class=p>:</span> <span class=mi>993</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;used_mb&#34;</span><span class=p>:</span> <span class=mi>450</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;percent&#34;</span><span class=p>:</span> <span class=mf>45.3</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;disk&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;total_gb&#34;</span><span class=p>:</span> <span class=mf>20.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;used_gb&#34;</span><span class=p>:</span> <span class=mf>8.4</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;percent&#34;</span><span class=p>:</span> <span class=mf>42.0</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;load_average&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;1m&#34;</span><span class=p>:</span> <span class=mf>0.42</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;5m&#34;</span><span class=p>:</span> <span class=mf>0.36</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;15m&#34;</span><span class=p>:</span> <span class=mf>0.30</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></li><li><code>GET /metrics/app</code>: m√©tricas da pr√≥pria aplica√ß√£o (em n√≠vel &ldquo;aplica√ß√£o&rdquo;).<ul><li>Exemplo:</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;uptime_seconds&#34;</span><span class=p>:</span> <span class=mi>1234</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;requests_count&#34;</span><span class=p>:</span> <span class=mi>87</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;startup_time&#34;</span><span class=p>:</span> <span class=s2>&#34;2025-11-15T15:00:00Z&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></li></ul><p>As seguintes depend√™ncias s√£o utilizadas como requisitos para que a API seja executada, estando localizadas em um arquivo <code>.txt</code> na raiz do reposit√≥rio do projeto:</p><pre tabindex=0><code>fastapi==0.121.3
uvicorn[standard]==0.38.0
psutil==7.1.3
python-dotenv==1.2.1
</code></pre><p>Para testar localmente, utilizei o <a href=https://uvicorn.dev/>Uvicorn</a>, uma implementa√ß√£o de servidor baseado no protocolo <a href=https://en.wikipedia.org/wiki/Asynchronous_Server_Gateway_Interface>ASGI</a>, em vistas de utilizar a se√ß√£o <code>/docs</code> do FastAPI:</p><pre tabindex=0><code>uvicorn app.main:app --reload
</code></pre><p>O reposit√≥rio com o c√≥digo da API pode ser acessado <a href=https://github.com/CassivsGabriellis/metrics-api-k8s-cluster-performance/tree/main/app>aqui</a>.</p><hr><h2 class=heading id=conteineriza√ß√£o-da-api-com-docker>Conteineriza√ß√£o da API com Docker
<a class=anchor href=#conteineriza%c3%a7%c3%a3o-da-api-com-docker>#</a></h2><p>Dando continuidade, d√°-se a conteineriza√ß√£o da API, pois todo o restante da arquitetura depende de uma imagem consistente, padronizada e facilmente replic√°vel. Para isso, crio um <strong>Dockerfile</strong> minimalista, baseado em uma imagem &ldquo;slim&rdquo; do Python 3.12, garantindo que o ambiente fosse leve, r√°pido para construir e adequado para rodar em m√°quinas com recursos limitados, como uma inst√¢ncia EC2 t3.small utilizada no Free Tier da AWS.</p><p>Dentro do Dockerfile, defino boas pr√°ticas essenciais como a configura√ß√£o das vari√°veis de ambiente <code>PYTHONDONTWRITEBYTECODE</code> e <code>PYTHONUNBUFFERED</code>, que reduzem o overhead de escrita em disco e melhoram a observabilidade de logs. Em seguida, instala-se as depend√™ncias de compila√ß√£o necess√°rias para pacotes como <code>psutil</code>, inclui-se o <code>requirements.txt</code> para instalar as depend√™ncias Python e copia-se o c√≥digo da pasta <code>app/</code> para dentro da imagem. Por fim, configuro o container para expor a porta 8000 e rodar o servidor Uvicorn, permitindo que a API FastAPI responda requisi√ß√µes HTTP dentro do cluster Kubernetes.</p><h3 class=heading id=arquivo-docker-na-raiz-do-projeto>Arquivo Docker na raiz do projeto
<a class=anchor href=#arquivo-docker-na-raiz-do-projeto>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-docker data-lang=docker><span class=line><span class=cl><span class=c># =========================</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># 1) Builder: installs deps</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># =========================</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>FROM</span><span class=w> </span><span class=s>python:3.12-slim</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=s>builder</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Better logging and no .pyc</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>ENV</span> <span class=nv>PYTHONDONTWRITEBYTECODE</span><span class=o>=</span><span class=m>1</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    <span class=nv>PYTHONUNBUFFERED</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>WORKDIR</span><span class=w> </span><span class=s>/app</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Build dependencies (psutil, etc.)</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> apt-get update <span class=o>&amp;&amp;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    apt-get install -y --no-install-recommends <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>        build-essential <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    <span class=o>&amp;&amp;</span> rm -rf /var/lib/apt/lists/*<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Install Python dependencies in a venv</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> requirements.txt .<span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> python -m venv /opt/venv <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    <span class=o>&amp;&amp;</span> /opt/venv/bin/pip install --upgrade pip <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    <span class=o>&amp;&amp;</span> /opt/venv/bin/pip install --no-cache-dir -r requirements.txt<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># =========================</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># 2) Runtime: final image</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># =========================</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>FROM</span><span class=w> </span><span class=s>python:3.12-slim</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=s>runtime</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>ENV</span> <span class=nv>PYTHONDONTWRITEBYTECODE</span><span class=o>=</span><span class=m>1</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    <span class=nv>PYTHONUNBUFFERED</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>WORKDIR</span><span class=w> </span><span class=s>/app</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Copy the already-prepared virtual environment</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> --from<span class=o>=</span>builder /opt/venv /opt/venv<span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>ENV</span> <span class=nv>PATH</span><span class=o>=</span><span class=s2>&#34;/opt/venv/bin:</span><span class=nv>$PATH</span><span class=s2>&#34;</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Copy only the application code</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> app ./app<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># API port</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>EXPOSE</span><span class=w> </span><span class=s>8000</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Default variables</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>ENV</span> <span class=nv>APP_ENV</span><span class=o>=</span>prod
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c># Command to run the FastAPI API</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>CMD</span> <span class=p>[</span><span class=s2>&#34;uvicorn&#34;</span><span class=p>,</span> <span class=s2>&#34;app.main:app&#34;</span><span class=p>,</span> <span class=s2>&#34;--host&#34;</span><span class=p>,</span> <span class=s2>&#34;0.0.0.0&#34;</span><span class=p>,</span> <span class=s2>&#34;--port&#34;</span><span class=p>,</span> <span class=s2>&#34;8000&#34;</span><span class=p>]</span><span class=err>
</span></span></span></code></pre></div><h3 class=heading id=build-e-push-manual-da-imagem-para-teste>Build e push manual da imagem (para teste)
<a class=anchor href=#build-e-push-manual-da-imagem-para-teste>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Build the image</span>
</span></span><span class=line><span class=cl>docker build -t cassiano00/metrics-api:latest .
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Test locally</span>
</span></span><span class=line><span class=cl>docker run --rm -p 8000:8000 cassiano00/metrics-api:latest
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Push to Docker Hub (for Kubernetes to pull)</span>
</span></span><span class=line><span class=cl>docker push cassiano00/metrics-api:latest
</span></span></code></pre></div><p><figure><div class=img-container style=--w:1473;--h:677><img loading=lazy alt src=/pt-br/posts/api-k8s-cluster/images/running-locally-docker.png width=1473 height=677></div></figure></p><hr><h2 class=heading id=orquestra√ß√£o-em-um-cluster-kubernetes>Orquestra√ß√£o em um cluster Kubernetes
<a class=anchor href=#orquestra%c3%a7%c3%a3o-em-um-cluster-kubernetes>#</a></h2><p>Dada a imagem montada, orquestro um cluster Kubernetes atrav√©s de manifests YAML. O primeiro componente criado foi o <code>namespace.yaml</code>, uma pr√°tica fundamental que organiza logicamente os recursos e evita conflitos entre servi√ßos diferentes dentro do cluster. Criar o namespace <strong>metrics-api</strong> garante isolamento e facilita futuras opera√ß√µes de gerenciamento e automa√ß√£o.</p><p>Para testar localmente, utilizo o <a href=https://minikube.sigs.k8s.io>Minikube</a>, que implementa um cluster Kubernetes local.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Namespace</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>metrics-api</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>metrics-api</span><span class=w>
</span></span></span></code></pre></div><p>Aplicando as configura√ß√µes presentes no arquivo <code>namespace.yaml</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl apply -f k8s/namespace.yaml
</span></span></code></pre></div><p><figure><div class=img-container style=--w:501;--h:174><img loading=lazy alt src=/pt-br/posts/api-k8s-cluster/images/kubeclt-command-1.png width=501 height=174></div></figure></p><p>Em seguida, definino o <code>deployment.yaml</code>, recurso central no Kubernetes respons√°vel por gerenciar e manter a aplica√ß√£o em execu√ß√£o de forma declarativa. No Deployment, configuro o lan√ßamento de 1 (uma) r√©plica, visto que √© um ambiente de teste. Especifico a imagem Docker constru√≠da anteriormente e adiciono <em>probes</em> de liveness e readiness apontando para o endpoint <code>/health</code>. Essas <em>probes</em> s√£o essenciais para que o Kubernetes detecte automaticamente falhas de containers e garanta que somente inst√¢ncias saud√°veis recebam tr√°fego. Al√©m disso, configuro <em>requests</em> e <em>limits</em> de CPU e mem√≥ria, evitando que o container consuma mais recursos do que deveria ‚Äî o que √© cr√≠tico em ambientes pequenos como um EC2 de baixo custo.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>apps/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Deployment</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>metrics-api-deployment</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>metrics-api</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>metrics-api</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>metrics-api</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>metrics-api</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>metrics-api</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>cassiano00/metrics-api:latest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>imagePullPolicy</span><span class=p>:</span><span class=w> </span><span class=l>IfNotPresent</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span>- <span class=nt>containerPort</span><span class=p>:</span><span class=w> </span><span class=m>8000</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>env</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>APP_ENV</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;prod&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>resources</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>requests</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span><span class=nt>cpu</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;100m&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span><span class=nt>memory</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;128Mi&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>limits</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span><span class=nt>cpu</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;500m&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span><span class=nt>memory</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;256Mi&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>readinessProbe</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>httpGet</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span><span class=nt>path</span><span class=p>:</span><span class=w> </span><span class=l>/health</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span><span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>8000</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>initialDelaySeconds</span><span class=p>:</span><span class=w> </span><span class=m>5</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>periodSeconds</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>timeoutSeconds</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>failureThreshold</span><span class=p>:</span><span class=w> </span><span class=m>3</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>livenessProbe</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>httpGet</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span><span class=nt>path</span><span class=p>:</span><span class=w> </span><span class=l>/health</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span><span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>8000</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>initialDelaySeconds</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>periodSeconds</span><span class=p>:</span><span class=w> </span><span class=m>20</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>timeoutSeconds</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>failureThreshold</span><span class=p>:</span><span class=w> </span><span class=m>3</span><span class=w>
</span></span></span></code></pre></div><p>Executando as configura√ß√µes:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl apply -f k8s/deployment.yaml
</span></span></code></pre></div><p><figure><div class=img-container style=--w:543;--h:88><img loading=lazy alt src=/pt-br/posts/api-k8s-cluster/images/kubeclt-command-2.png width=543 height=88></div></figure></p><p>Com o Deployment definido, crio um <code>service.yaml</code> do tipo <strong>ClusterIP</strong> para fornecer um <em>endpoint</em> interno est√°vel que abstrai os pods. O Service exp√µe a porta <code>80</code> internamente e repassa chamadas para a porta <code>8000</code> do container, padronizando o acesso e permitindo que outros componentes, como o <a href=https://kubernetes.io/docs/concepts/services-networking/ingress/>Ingress</a>, interajam com o backend sem depender da estrutura interna do Deployment. Esta √© uma boa pr√°tica de isolamento em clusters.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Service</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>metrics-api-service</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>metrics-api</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>metrics-api</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>metrics-api</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>http</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>targetPort</span><span class=p>:</span><span class=w> </span><span class=m>8000</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>ClusterIP</span><span class=w>
</span></span></span></code></pre></div><p>Aplicando as configura√ß√µes:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl apply -f k8s/service.yaml
</span></span></code></pre></div><p><figure><div class=img-container style=--w:568;--h:76><img loading=lazy alt src=/pt-br/posts/api-k8s-cluster/images/kubeclt-command-3.png width=568 height=76></div></figure></p><p>Finalizando a camada Kubernetes, implemento o arquivo <code>ingress.yaml</code>, respons√°vel por fornecer uma interface HTTP externa ao cluster por meio do NGINX Ingress Controller. No ambiente local, onde o Minikube est√° sendo executado com driver Docker no Windows, o acesso direto ao IP interno do cluster n√£o √© poss√≠vel. Por isso, utilizo o <strong><code>minikube tunnel</code></strong>, que cria uma rota entre o host e o Ingress Controller, expondo o tr√°fego de entrada de forma confi√°vel em <code>127.0.0.1</code>.</p><p>Com essa configura√ß√£o, o Ingress atua como o ponto de entrada oficial da aplica√ß√£o dentro do cluster, mapeando requisi√ß√µes externas para o Service interno (<code>metrics-api-service:80</code>). Isso remove a necessidade de t√∫neis tempor√°rios como o <code>minikube service ... --url</code> e garante um fluxo de tr√°fego id√™ntico ao utilizado em ambientes reais: cliente ‚Üí Ingress NGINX ‚Üí Service ‚Üí Pods.</p><p>Ao centralizar o acesso externo no Ingress, a arquitetura se torna mais organizada, escal√°vel e alinhada ao padr√£o utilizado em clusters Kubernetes de produ√ß√£o. Esse componente tamb√©m habilita futuras extens√µes ‚Äî como suporte a TLS, autentica√ß√£o, rate limiting e roteamento avan√ßado. Al√©m disso, prepara naturalmente o ambiente para a futura integra√ß√£o com Prometheus e Grafana, facilitando a exposi√ß√£o de m√©tricas, dashboards e observabilidade completa da aplica√ß√£o.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>networking.k8s.io/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Ingress</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>metrics-api-ingress</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>metrics-api</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>kubernetes.io/ingress.class</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;nginx&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>rules</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>http</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>paths</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span>- <span class=nt>path</span><span class=p>:</span><span class=w> </span><span class=l>/</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>pathType</span><span class=p>:</span><span class=w> </span><span class=l>Prefix</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>backend</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span><span class=nt>service</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>metrics-api-service</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                </span><span class=nt>port</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                  </span><span class=nt>number</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span></span></span></code></pre></div><p>Executando as configura√ß√µes:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl apply -f k8s/ingress.yaml
</span></span></code></pre></div><p><figure><div class=img-container style=--w:516;--h:75><img loading=lazy alt src=/pt-br/posts/api-k8s-cluster/images/kubeclt-command-4.png width=516 height=75></div></figure></p><p>Testando o endpoint via comando <code>curl</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl http://127.0.0.1/health
</span></span><span class=line><span class=cl>curl http://127.0.0.1/metrics/system
</span></span></code></pre></div><p><figure><div class=img-container style=--w:1335;--h:275><img loading=lazy alt src=/pt-br/posts/api-k8s-cluster/images/kubeclt-command-5.png width=1335 height=275></div></figure></p><hr><h2 class=heading id=provisionando-a-infraestrutura-via-terraform>Provisionando a infraestrutura via Terraform
<a class=anchor href=#provisionando-a-infraestrutura-via-terraform>#</a></h2><p>Ao avan√ßar para a etapa de infraestrutura do projeto, minha prioridade foi estabelecer uma base s√≥lida e reprodut√≠vel para executar o cluster Kubernetes que futuramente hospedar√° a API de m√©tricas. Para isso, iniciei pelo provisionamento da camada de rede e computa√ß√£o utilizando Terraform. O objetivo era garantir que toda a funda√ß√£o da aplica√ß√£o ‚Äî desde a VPC at√© a inst√¢ncia EC2 ‚Äî fosse criada de forma declarativa, audit√°vel e consistente. Criei uma VPC dedicada, uma sub-rede p√∫blica e uma tabela de rotas conectada a um Internet Gateway, assegurando que a inst√¢ncia tivesse acesso √† internet para instalar depend√™ncias, baixar imagens e operar o k3s sem restri√ß√µes. Em seguida, configurei um Security Group com regras estritamente necess√°rias: acesso SSH para administra√ß√£o e portas HTTP/HTTPS abertas para o futuro Ingress Controller. Feito isso, defini uma inst√¢ncia EC2 <strong>t3.small</strong> ‚Äî suficiente para um ambiente de valida√ß√£o ‚Äî utilizando a AMI do Ubuntu 22.04, garantindo compatibilidade com Docker, k3s e demais ferramentas da stack.</p><p>Os seguinte itens ser√£o provisionados em nuvem:</p><ul><li>1 VPC</li><li>1 public subnet</li><li>Internet gateway + route table</li><li>1 ElasticIP (fixed public Ip)</li><li>1 Security Group (SSH + HTTP/HTTPS)</li><li>1 inst√¢ncia EC2 (t3.small) que servir√° como um node <a href=https://docs.k3s.io/>k3s</a></li></ul><h3 class=heading id=configurando-a-vpc>Configurando a VPC
<a class=anchor href=#configurando-a-vpc>#</a></h3><p>Comecei criando uma VPC dedicada com um bloco CIDR amplo (<code>10.0.0.0/16</code>), permitindo flexibilidade para expans√£o futura de sub-redes, balanceadores ou n√≥s adicionais. Em seguida, configurei uma sub-rede p√∫blica (<code>10.0.1.0/24</code>) com <code>map_public_ip_on_launch</code> habilitado, garantindo que inst√¢ncias dentro dessa sub-rede recebessem um IP p√∫blico automaticamente, eliminando a necessidade de Elastic IPs no ambiente de valida√ß√£o. Associei essa sub-rede a uma tabela de rotas contendo a rota padr√£o (<code>0.0.0.0/0</code>) direcionada para um Internet Gateway rec√©m-criado, assegurando conectividade externa total ‚Äî algo essencial para que o n√≥ pudesse baixar pacotes, realizar pull de imagens Docker e se comunicar com registries p√∫blicos. Tamb√©m provisionei um endere√ßo IPv4 est√°tico, atrav√©s de um ElasticIP associado √† inst√¢ncia.</p><p><strong><code>main.tf</code></strong>:</p><pre tabindex=0><code># VPC
resource &#34;aws_vpc&#34; &#34;this&#34; {
  cidr_block = &#34;10.0.0.0/16&#34;
    tags = {
        Name = &#34;${var.project_name}-vpc&#34;
    }
  enable_dns_hostnames = true
  enable_dns_support = true
}

# Public Subnet
resource &#34;aws_subnet&#34; &#34;public&#34; {
  vpc_id                    = aws_vpc.this.id
  cidr_block                = &#34;10.0.1.0/24&#34;
  map_public_ip_on_launch   = true

  tags = {
    Name = &#34;${var.project_name}-public-subnet&#34;
  }
}

# Public Route Table for subnet
resource &#34;aws_route_table&#34; &#34;public&#34; {
  vpc_id = aws_vpc.this.id

  route {
    cidr_block = &#34;0.0.0.0/0&#34;
    gateway_id = aws_internet_gateway.this.id
  }
    tags = {
        Name = &#34;${var.project_name}-public-rt&#34;
    }
}

...

# Elastic IP associated with the k3s_node instance
resource &#34;aws_eip&#34; &#34;k3s_eip&#34; {
  domain   = &#34;vpc&#34;
  instance = aws_instance.k3s_node.id

  tags = {
    Name = &#34;${var.project_name}-eip&#34;
  }
}
</code></pre><h3 class=heading id=configurando-um-security-group>Configurando um Security Group
<a class=anchor href=#configurando-um-security-group>#</a></h3><p>Na parte de seguran√ßa, constru√≠ um Security Group seguindo o princ√≠pio do menor privil√©gio, liberando apenas o tr√°fego necess√°rio: porta 22 para SSH (limitada via vari√°vel parametriz√°vel), porta 80 para receber tr√°fego HTTP futuramente via Ingress e porta 443 para antecipar cen√°rios de TLS. Todo o restante permaneceu bloqueado.</p><p><strong><code>main.tf</code></strong>:</p><pre tabindex=0><code># Security Group for EC2 instance
resource &#34;aws_security_group&#34; &#34;ec2_sg&#34; {
  name        = &#34;${var.project_name}-ec2-sg&#34;
  description = &#34;Security group for metrics API k3s node&#34;
  vpc_id      = aws_vpc.this.id

  # SSH access
  ingress {
    description      = &#34;Allow SSH&#34;
    from_port        = 22
    to_port          = 22
    protocol         = &#34;tcp&#34;
    cidr_blocks      = [var.allowed_ssh_cidr]
  }
  
  # HTTP access
  ingress {
    description      = &#34;Allow HTTP&#34;
    from_port        = 80
    to_port          = 80
    protocol         = &#34;tcp&#34;
    cidr_blocks      = [&#34;0.0.0.0/0&#34;]
  }

  # HTTPS (for future TLS)
  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = &#34;tcp&#34;
    cidr_blocks = [&#34;0.0.0.0/0&#34;]
  }

  # Egress: allow everything
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = &#34;-1&#34;
    cidr_blocks = [&#34;0.0.0.0/0&#34;]
  }

  tags = {
    Name = &#34;${var.project_name}-ec2-sg&#34;
  }
}
</code></pre><h3 class=heading id=configurando-a-inst√¢ncia-ec2>Configurando a inst√¢ncia EC2
<a class=anchor href=#configurando-a-inst%c3%a2ncia-ec2>#</a></h3><p>Com a rede estabelecida, finalizei o provisionamento criando uma inst√¢ncia EC2 <code>t3.small</code>, suficiente para cen√°rios de teste, utilizando a AMI oficial do Ubuntu 22.04. Essa escolha foi deliberada, dadas suas otimiza√ß√µes, compatibilidade plena com Docker e suporte nativo a <strong>systemd</strong> ‚Äî fundamental para o funcionamento adequado dos servi√ßos do k3s. Assim, toda a camada fundacional do cluster estava definida n√£o apenas de manera declarativa, mas tamb√©m calibrada para operar workloads containerizados.</p><p><strong><code>main.tf</code></strong>:</p><pre tabindex=0><code># Ubuntu AMI
data &#34;aws_ami&#34; &#34;ubuntu&#34; {
  most_recent = true
  owners      = [&#34;099720109477&#34;] # Canonical

  filter {
    name   = &#34;name&#34;
    values = [&#34;ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*&#34;]
  }

  filter {
    name   = &#34;virtualization-type&#34;
    values = [&#34;hvm&#34;]
  }
}

# EC2 instance that will run k3s
resource &#34;aws_instance&#34; &#34;k3s_node&#34; {
  ami                         = data.aws_ami.ubuntu.id
  instance_type               = &#34;t3.small&#34;
  subnet_id                   = aws_subnet.public.id
  vpc_security_group_ids      = [aws_security_group.ec2_sg.id]
  key_name                    = var.key_name
  associate_public_ip_address = false 

  tags = {
    Name = &#34;${var.project_name}-k3s-node&#34;
  }
}
</code></pre><p>Antes deste processo, j√° havia configurado meu ambiente com o <a href=https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html>AWS CLI</a>, para que as devidas permiss√µes de conex√£o com meu perfil AWS fossem estabelecidas.</p><p>Tamb√©m tive que criar uma <strong>key pair</strong> na regi√£o <code>sa-east-1</code> para que o Terraform pudesse aplicar as configura√ß√µes para cria√ß√£o da inst√¢ncia EC2, esta que se encontra como <code>key_name = var.key_name</code>.</p><p>Assim, iniciando o ambiente Terraform no diret√≥rio <code>/infra/terraform</code> e aplicando as configura√ß√µes estabelecidas:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>terraform init
</span></span><span class=line><span class=cl>terraform apply
</span></span></code></pre></div><p>O ambiente foi devidamente provisionado na AWS:</p><p><figure><div class=img-container style=--w:824;--h:200><img loading=lazy alt src=/pt-br/posts/api-k8s-cluster/images/terraform-apply-config.png width=824 height=200></div></figure><figure><div class=img-container style=--w:1648;--h:764><img loading=lazy alt src=/pt-br/posts/api-k8s-cluster/images/created-instance-after-terraform-apply.png width=1648 height=764></div></figure></p><p>O restante das configura√ß√µes em c√≥digo do Terraform podem ser acessadas <a href=https://github.com/CassivsGabriellis/metrics-api-k8s-cluster-performance/tree/main/infra/terraform>aqui</a>.</p><hr><h2 class=heading id=implementando-as-configura√ß√µes-com-ansible>Implementando as configura√ß√µes com Ansible
<a class=anchor href=#implementando-as-configura%c3%a7%c3%b5es-com-ansible>#</a></h2><p>Com a infraestrutura provisionada, avancei para a etapa de automa√ß√£o da configura√ß√£o do n√≥ Kubernetes utilizando Ansible. Minha meta era transformar uma inst√¢ncia rec√©m-criada em um n√≥ Kubernetes funcional, pronto para receber workloads, sem qualquer configura√ß√£o manual. Estruturei um playbook que executa desde a atualiza√ß√£o de pacotes e instala√ß√£o de depend√™ncias base, at√© a configura√ß√£o completa do runtime de containers e da pr√≥pria distribui√ß√£o k3s. Instalei o Docker para garantir suporte a workloads baseados em containerd e compatibilidade com ferramentas de desenvolvimento, e, em seguida, executei o script oficial de instala√ß√£o do k3s com op√ß√µes espec√≠ficas ‚Äî incluindo a desativa√ß√£o do Traefik, preservando o cluster limpo para implementa√ß√µes posteriores.</p><p><strong><code>playbook.yaml</code></strong>:</p><pre tabindex=0><code>---
- name: Configure EC2 as k3s Kubernetes node
  hosts: k3s_node
  become: true
  vars:
    k3s_version: &#34;&#34; 
  tasks:
    - name: Update apt cache
      apt:
        update_cache: yes
        cache_valid_time: 3600

    - name: Install base packages
      apt:
        name:
          - curl
          - wget
          - git
        state: present

    - name: Install Docker
      apt:
        name:
          - docker.io
        state: present

    - name: Enable and start Docker
      systemd:
        name: docker
        state: started
        enabled: true

    - name: Add ubuntu user to docker group
      user:
        name: ubuntu
        groups: docker
        append: yes

    - name: Download and install k3s
      shell: |
        curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=&#34;server --disable traefik&#34; sh -
      args:
        creates: /usr/local/bin/k3s

    - name: Install NGINX Ingress Controller
      become: false
      command: &gt;
        kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.14.0/deploy/static/provider/cloud/deploy.yaml

    - name: Wait for k3s service to be active
      systemd:
        name: k3s
        state: started
        enabled: true

    - name: Ensure .kube directory exists for ubuntu user
      file:
        path: /home/ubuntu/.kube
        state: directory
        owner: ubuntu
        group: ubuntu
        mode: &#34;0700&#34;

    - name: Copy k3s kubeconfig to ubuntu user
      copy:
        src: /etc/rancher/k3s/k3s.yaml
        dest: /home/ubuntu/.kube/config
        owner: ubuntu
        group: ubuntu
        mode: &#34;0600&#34;
        remote_src: yes
    
    - name: Export KUBECONFIG in ubuntu .bashrc
      lineinfile:
        path: /home/ubuntu/.bashrc
        regexp: &#39;^export KUBECONFIG=&#39;
        line: &#39;export KUBECONFIG=$HOME/.kube/config&#39;
        create: yes
        owner: ubuntu
        group: ubuntu
        mode: &#34;0644&#34;

    - name: Create kubectl symlink for convenience
      file:
        src: /usr/local/bin/kubectl
        dest: /usr/local/bin/k3s-kubectl
        state: link
      ignore_errors: yes

    - name: Ensure kubectl installed (k3s includes it)
      file:
        src: /usr/local/bin/k3s
        dest: /usr/local/bin/kubectl
        state: link
      ignore_errors: yes
</code></pre><p>Junto com as especifica√ß√µes do meu host no arquivo local <strong><code>host_vars/k3s-ec2-node.yaml</code></strong>:</p><pre tabindex=0><code>ansible_host: &lt;instance-public-ip&gt;
ansible_user: ubuntu
ansible_ssh_private_key_file: &lt;metrics-api-key.pem-local-address&gt;
</code></pre><p><strong><code>inventory.ini</code></strong>:</p><pre tabindex=0><code>[k3s_node]
k3s-ec2-node
</code></pre><p>Assim, com as configura√ß√µes dadas, executo o comando para o Ansible aplic√°-las:</p><pre tabindex=0><code>ansible-playbook -i inventory.ini playbook.yaml
</code></pre><p><figure><div class=img-container style=--w:1395;--h:956><img loading=lazy alt src=/pt-br/posts/api-k8s-cluster/images/ansible-config-success.png width=1395 height=956></div></figure></p><p>Ap√≥s a instala√ß√£o do k3s via Ansible, mantive a kubeconfig padr√£o gerada pelo servi√ßo em <code>/etc/rancher/k3s/k3s.yaml</code>, sem modificar o endpoint <code>127.0.0.1:6443</code>, uma vez que a administra√ß√£o do cluster ser√° realizada diretamente dentro da pr√≥pria inst√¢ncia EC2, utilizando <code>kubectl</code>. Durante a automa√ß√£o, o playbook cuidou da instala√ß√£o do Docker, da ativa√ß√£o do k3s como servi√ßo, da configura√ß√£o dos bin√°rios e symlinks necess√°rios para o uso do cliente kubectl, garantindo que todos os utilit√°rios essenciais estivessem dispon√≠veis para o usu√°rio padr√£o da m√°quina. Ao final dessa etapa, a inst√¢ncia EC2 j√° operava como um n√≥ Kubernetes funcional, inicializado e pronto para receber aplica√ß√µes e deployments automatizados via pipeline CI/CD.</p><p>Para verificar o estado atual da inst√¢ncia, fiz um acesso via SSH, para averiguar o node sendo executado dentro dela:<figure><div class=img-container style=--w:966;--h:89><img loading=lazy alt src=/pt-br/posts/api-k8s-cluster/images/ssh-instance-access.png width=966 height=89></div></figure></p><hr><h2 class=heading id=pipeline-cicd-com-github-actions>Pipeline CI/CD com GitHub Actions
<a class=anchor href=#pipeline-cicd-com-github-actions>#</a></h2><p>Com a infraestrutura provisionada e o cluster k3s operacional na EC2, √© hora de projetar uma pipeline CI/CD totalmente automatizada usando GitHub Actions. O objetivo √© eliminar qualquer necessidade de interven√ß√£o manual tanto no build quanto no deploy, garantindo que toda altera√ß√£o no c√≥digo da API resulte em uma nova vers√£o do servi√ßo executando no cluster de forma imediata, previs√≠vel e confi√°vel.</p><h3 class=heading id=bootstrap-do-cluster-na-inst√¢ncia-ec2>Bootstrap do cluster na inst√¢ncia EC2
<a class=anchor href=#bootstrap-do-cluster-na-inst%c3%a2ncia-ec2>#</a></h3><p>Antes de continuar com a cri√ß√£o do fluxo CI/CD, √© importante notar que a inst√¢ncia encontra-se atualmente sem os objetos Kubernetes na EC2. Se for rodar o comando <code>kubectl get ns</code> e <code>kubectl get all -n metrics-api</code>, o resultado ser√° que <code>metrics-api</code> n√£o aparece na lista de <strong>namespaces</strong> (forma de organizar e isolar recursos dentro de um cluster) e nada existe em <code>-n metrics-api</code> (porque o namespace n√£o existe).</p><p><figure><div class=img-container style=--w:938;--h:274><img loading=lazy alt src=/pt-br/posts/api-k8s-cluster/images/no-namespace.png width=938 height=274></div></figure></p><p>Dessa forma, √© necess√°rio:</p><ul><li>Criar o namespace <code>metrics-api</code>;</li><li>Criar o Deployment <code>metrics-api-deployment</code>, o Service, e o Ingress.</li></ul><p>Para isso, farei uma bootstrap manual na EC2, ou seja, uma inicializa√ß√£o e configura√ß√£o da inst√¢ncia de forma manual.</p><ol><li><p>Copiei os <em>manifests</em> Kubernetes da pasta <code>/k8s</code> para dentro da EC2 (do seu WSL):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>scp -i ~/.ssh/metrics-api-key.pem -r k8s ubuntu@&lt;public-ip-instance&gt;:/home/ubuntu/k8s
</span></span></code></pre></div></li></ol><p><figure><div class=img-container style=--w:1098;--h:122><img loading=lazy alt src=/pt-br/posts/api-k8s-cluster/images/scp-to-instance.png width=1098 height=122></div></figure></p><ol start=2><li><p>Apois isso, acessei √† inst√¢ncia EC2, para verificar se a pasta estava presente:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ssh -i ~/.ssh/metrics-api-key.pem ubuntu@&lt;public-ip-instance&gt;
</span></span></code></pre></div></li></ol><p><figure><div class=img-container style=--w:916;--h:140><img loading=lazy alt src=/pt-br/posts/api-k8s-cluster/images/k8s-inside-instance.png width=916 height=140></div></figure></p><ol start=3><li><p>Apliquei os <em>manifests</em> dentro da inst√¢ncia:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl apply -f /home/ubuntu/k8s/namespace.yaml
</span></span><span class=line><span class=cl>kubectl apply -f /home/ubuntu/k8s/deployment.yaml
</span></span><span class=line><span class=cl>kubectl apply -f /home/ubuntu/k8s/service.yaml
</span></span><span class=line><span class=cl>kubectl apply -f /home/ubuntu/k8s/ingress.yaml
</span></span></code></pre></div></li><li><p>E verifiquei se os <em>manifests</em> dentro da inst√¢ncia est√£o rodando:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl get ns
</span></span><span class=line><span class=cl>kubectl get deploy -n metrics-api
</span></span><span class=line><span class=cl>kubectl get pods -n metrics-api
</span></span><span class=line><span class=cl>kubectl get svc -n metrics-api
</span></span></code></pre></div></li></ol><p>Desta forma:</p><ul><li>O namespace <code>metrics-api</code> existir√°</li><li>O <code>metrics-api-deployment</code> existir√°</li><li>O Service e o Ingress existir√£o</li></ul><p>Assim, na pr√≥xima execu√ß√£o do GitHub Actions, o comando:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>kubectl -n metrics-api <span class=nb>set</span> image deployment/metrics-api-deployment <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  metrics-api<span class=o>=</span><span class=si>${</span><span class=nv>IMAGE</span><span class=si>}</span>
</span></span></code></pre></div><p>vai funcionar, pois as especifica√ß√µes do deployment j√° existem e s√≥ precisam da imagem Docker.</p><p><figure><div class=img-container style=--w:887;--h:562><img loading=lazy alt src=/pt-br/posts/api-k8s-cluster/images/applied-manifests-inside-instance.png width=887 height=562></div></figure></p><p>Os seguintes <code>secrets</code> do Reposit√≥rio foram estabelecidos para serem chamados no arquivo <code>ci-cd.yaml</code>:<figure><div class=img-container style=--w:794;--h:357><img loading=lazy alt src=/pt-br/posts/api-k8s-cluster/images/github-repo-secrets.png width=794 height=357></div></figure></p><ul><li><code>DOCKERHUB_USERNAME</code> - nome de usu√°rio no Docker Hub</li><li><code>DOCKERHUB_TOKEN</code> - token de acesso/senha do Docker Hub</li><li><code>EC2_HOST</code> - IP p√∫blico da inst√¢ncia (valor <code>ec2_public_ip</code> gerado pelo Terraform)</li><li><code>EC2_SSH_USER</code> - escolhi <code>ubuntu</code> para AMIs do Ubuntu</li><li><code>EC2_SSH_KEY</code> - conte√∫do da chave privada para SSH (arquivo <code>.pem</code>)</li></ul><p>Esse processo inicial cria a estrutura base do cluster ‚Äî incluindo o namespace <code>metrics-api</code> e os recursos essenciais ‚Äî permitindo que o pipeline automatizado trabalhe sobre uma funda√ß√£o j√° existente. Uma vez que esses objetos iniciais est√£o aplicados no cluster, todas as atualiza√ß√µes subsequentes podem ser controladas exclusivamente pelo CI/CD, sem necessidade de reaplicar manifests.</p><p>Com essa funda√ß√£o criada, desenvolvi o pipeline do GitHub Actions dividido em duas fases:</p><ol><li><p><strong>Build e Push da imagem Docker:</strong>
O workflow inicia realizando checkout do reposit√≥rio e configurando o Docker Buildx. Em seguida, constr√≥i a imagem da API e realiza o push para o Docker Hub utilizando duas tags: <code>latest</code>, destinada ao ambiente de desenvolvimento cont√≠nuo, e uma tag imut√°vel baseada no SHA do commit, assegurando rastreabilidade e versionamento confi√°vel. Essa abordagem garante que cada build seja reproduz√≠vel e associado a um ponto espec√≠fico da evolu√ß√£o do c√≥digo.</p></li><li><p><strong>Deploy autom√°tico no cluster k3s:</strong>
Na segunda etapa, o pipeline estabelece uma conex√£o SSH com a inst√¢ncia EC2 e utiliza <code>kubectl set image</code> para atualizar o Deployment existente com a nova imagem publicada. Como o cluster j√° possui todos os manifests aplicados no bootstrap inicial, o pipeline precisa apenas ajustar a imagem do Deployment, tornando o processo r√°pido, eficiente e sem duplica√ß√£o de recursos. Ap√≥s a atualiza√ß√£o, o workflow aguarda o rollout para confirmar que a nova vers√£o foi aplicada com sucesso.</p></li></ol><p><figure><div class=img-container style=--w:1882;--h:856><img loading=lazy alt src=/pt-br/posts/api-k8s-cluster/images/github-build-push-deploy.png width=1882 height=856></div></figure></p><p>Assim, a pipeline completa ‚Äî do c√≥digo √† atualiza√ß√£o em produ√ß√£o ‚Äî tornou-se totalmente automatizada, determin√≠stica e alinhada √†s melhores pr√°ticas modernas de CI/CD em ambientes Kubernetes.</p><p>O c√≥digo na √≠ntegra da pipeline no GitHub Actions pode ser acesso <a href=https://github.com/CassivsGabriellis/metrics-api-k8s-cluster-performance/blob/main/.github/workflows/ci-cd.yaml>aqui</a>.</p><h3 class=heading id=testes-realizados-dentro-da-inst√¢ncia-ec2-ap√≥s-deploy>Testes realizados dentro da inst√¢ncia EC2 ap√≥s deploy
<a class=anchor href=#testes-realizados-dentro-da-inst%c3%a2ncia-ec2-ap%c3%b3s-deploy>#</a></h3><h4 class=heading id=vis√£o-geral-do-node-recursos-no-namespace-da-api-e-listagem-dos-pods>Vis√£o geral do node, recursos no namespace da API e listagem dos pods
<a class=anchor href=#vis%c3%a3o-geral-do-node-recursos-no-namespace-da-api-e-listagem-dos-pods>#</a></h4><p><figure><div class=img-container style=--w:1905;--h:466><img loading=lazy alt src=/pt-br/posts/api-k8s-cluster/images/after-github-deploy-1.png width=1905 height=466></div></figure></p><h4 class=heading id=teste-nos-endpoints-da-api-via-curl-dentro-da-inst√¢ncia-ec2>Teste nos endpoints da API via <code>curl</code> dentro da inst√¢ncia EC2
<a class=anchor href=#teste-nos-endpoints-da-api-via-curl-dentro-da-inst%c3%a2ncia-ec2>#</a></h4><p><figure><div class=img-container style=--w:1915;--h:560><img loading=lazy alt src=/pt-br/posts/api-k8s-cluster/images/after-github-deploy-2.png width=1915 height=560></div></figure></p><h4 class=heading id=acessando-a-se√ß√£o-docs-do-fastapi-via-ip-p√∫blica-da-inst√¢ncia-ec2>Acessando a se√ß√£o <code>docs</code> do FastAPI via IP p√∫blica da inst√¢ncia EC2
<a class=anchor href=#acessando-a-se%c3%a7%c3%a3o-docs-do-fastapi-via-ip-p%c3%bablica-da-inst%c3%a2ncia-ec2>#</a></h4><p><figure><div class=img-container style=--w:1917;--h:862><img loading=lazy alt src=/pt-br/posts/api-k8s-cluster/images/docs-fastapi-public-api.png width=1917 height=862></div></figure></p><hr><h2 class=heading id=sum√°rio-acesso-√†-api-na-ec2-e-utiliza√ß√£o-pr√°tica-da-inst√¢ncia>Sum√°rio: Acesso √† API na EC2 e utiliza√ß√£o pr√°tica da inst√¢ncia
<a class=anchor href=#sum%c3%a1rio-acesso-%c3%a0-api-na-ec2-e-utiliza%c3%a7%c3%a3o-pr%c3%a1tica-da-inst%c3%a2ncia>#</a></h2><p>Com o provisionamento consolidado, a inst√¢ncia EC2 deixa de ser apenas um ‚Äún√≥ de infraestrutura‚Äù e passa a atuar como ponto de entrada est√°vel para a API de m√©tricas. A seguir, descrevo como ela est√° exposta, como pode ser consumida externamente e como pode servir de base para extens√µes futuras do projeto.</p><h3 class=heading id=caracter√≠sticas-da-inst√¢ncia-e-do-endpoint-p√∫blico>Caracter√≠sticas da inst√¢ncia e do endpoint p√∫blico
<a class=anchor href=#caracter%c3%adsticas-da-inst%c3%a2ncia-e-do-endpoint-p%c3%bablico>#</a></h3><p>A inst√¢ncia EC2 foi provisionada como um n√≥ √∫nico de Kubernetes com k3s, utilizando o tipo <code>t3.small</code>, o que garante 2 vCPUs e 2 GiB de RAM ‚Äî um equil√≠brio melhor entre custo e capacidade para rodar o cluster, o Ingress Controller e a aplica√ß√£o simultaneamente.</p><p>Do ponto de vista de rede, a inst√¢ncia est√° em:</p><ul><li><p>Uma VPC dedicada (<code>10.0.0.0/16</code>), com:</p><ul><li>Sub-rede p√∫blica (<code>10.0.1.0/24</code>) para o n√≥ k3s;</li><li>Internet Gateway associado √† VPC;</li><li>Tabela de rotas p√∫blica com sa√≠da <code>0.0.0.0/0</code> apontando para o IGW;</li></ul></li><li><p>Um Security Group espec√≠fico para o n√≥ k3s, permitindo:</p><ul><li>SSH (porta 22) apenas a partir do CIDR definido em <code>allowed_ssh_cidr</code>;</li><li>HTTP (porta 80) aberto para <code>0.0.0.0/0</code>, para acesso p√∫blico √† API;</li><li>HTTPS (porta 443) aberto para futuros cen√°rios com TLS.</li></ul></li></ul><p>Al√©m disso, a inst√¢ncia utiliza um <strong>Elastic IP</strong>, provisionado via Terraform e exposto por meio do output <code>ec2_public_ip</code>. Isso garante que:</p><ul><li>O IPv4 p√∫blico da inst√¢ncia permanece est√°vel entre reinicializa√ß√µes;</li><li>O pipeline CI/CD e quaisquer clientes externos podem apontar para um endere√ßo fixo, sem necessidade de atualizar configura√ß√µes a cada stop/start.</li></ul><p>Na pr√°tica, √© esse Elastic IP que funciona como ‚Äúendpoint p√∫blico bruto‚Äù da API.</p><h3 class=heading id=exposi√ß√£o-da-api-via-ingress-nginx>Exposi√ß√£o da API via Ingress NGINX
<a class=anchor href=#exposi%c3%a7%c3%a3o-da-api-via-ingress-nginx>#</a></h3><p>Internamente, o tr√°fego HTTP √© roteado pelo <strong>NGINX Ingress Controller</strong>, instalado no cluster k3s. A topologia l√≥gica fica assim:</p><pre tabindex=0><code>Cliente ‚Üí Elastic IP (port 80) ‚Üí EC2 (Security Group) 
        ‚Üí ingress-nginx Service LoadBalancer
        ‚Üí Ingress (rules / path /) 
        ‚Üí Service ClusterIP (metrics-api-service) 
        ‚Üí API pod (container FastAPI on the port 8000)
</code></pre><p>O recurso <code>Ingress</code> configurado para o namespace <code>metrics-api</code> faz o roteamento da raiz <code>/</code> para o <code>metrics-api-service</code>, que por sua vez encaminha as requisi√ß√µes para o Deployment <code>metrics-api-deployment</code>. Isso significa que, externamente, o consumo da API pode ser feito de forma direta, utilizando o Elastic IP na porta 80:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl http://&lt;elastic-ip&gt;/health
</span></span></code></pre></div><p>Esse endpoint <code>/health</code> √© exposto pela aplica√ß√£o FastAPI e funciona como uma verifica√ß√£o simples de disponibilidade do servi√ßo. Dessa forma, qualquer cliente HTTP ‚Äî desde scripts de monitoramento at√© ferramentas de observabilidade ‚Äî pode utilizar esse endere√ßo para valida√ß√µes b√°sicas ou integra√ß√µes com sondas de sa√∫de.</p><h3 class=heading id=utiliza√ß√£o-da-inst√¢ncia-para-inspe√ß√£o-debug-e-opera√ß√£o>Utiliza√ß√£o da inst√¢ncia para inspe√ß√£o, debug e opera√ß√£o
<a class=anchor href=#utiliza%c3%a7%c3%a3o-da-inst%c3%a2ncia-para-inspe%c3%a7%c3%a3o-debug-e-opera%c3%a7%c3%a3o>#</a></h3><p>Al√©m de servir a API externamente, a inst√¢ncia tamb√©m √© o ponto central para opera√ß√µes administrativas e de troubleshooting. A partir de uma sess√£o SSH utilizando a chave privada (<code>EC2_SSH_KEY</code>) e o usu√°rio configurado (<code>EC2_SSH_USER</code>, no caso <code>ubuntu</code>), √© poss√≠vel:</p><ul><li><p>Inspecionar o estado do cluster:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl get nodes -o wide
</span></span><span class=line><span class=cl>kubectl get all -n metrics-api
</span></span></code></pre></div></li><li><p>Acompanhar o comportamento dos pods da aplica√ß√£o:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl get pods -n metrics-api -o wide
</span></span><span class=line><span class=cl>kubectl logs -n metrics-api &lt;nome-do-pod&gt;
</span></span></code></pre></div></li><li><p>Testar a API de dentro do cluster, seja via port-forward, seja executando comandos dentro do pod:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Port-forward of the Service to the local machine (inside EC2)</span>
</span></span><span class=line><span class=cl>kubectl port-forward svc/metrics-api-service -n metrics-api 8000:80
</span></span><span class=line><span class=cl>curl http://127.0.0.1:8000/health
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Direct call from inside the pod</span>
</span></span><span class=line><span class=cl>kubectl <span class=nb>exec</span> -it -n metrics-api &lt;pod-name&gt; -- curl -s http://127.0.0.1:8000/health
</span></span></code></pre></div></li></ul><p>Isso transforma a inst√¢ncia em um ponto √∫nico de observa√ß√£o do ciclo de vida da aplica√ß√£o: desde o n√≠vel Kubernetes (Deployments, Pods, Services, Ingress) at√© o n√≠vel de aplica√ß√£o (logs, respostas HTTP, status de sa√∫de).</p><h3 class=heading id=extens√µes-futuras-observabilidade-tls-e-dom√≠nios-customizados>Extens√µes futuras: observabilidade, TLS e dom√≠nios customizados
<a class=anchor href=#extens%c3%b5es-futuras-observabilidade-tls-e-dom%c3%adnios-customizados>#</a></h3><p>A forma como a inst√¢ncia foi desenhada permite uma s√©rie de evolu√ß√µes naturais, sem necessidade de reestruturar a base:</p><ul><li><p><strong>Integra√ß√£o com Prometheus e Grafana:</strong>
O cluster k3s atual pode receber um stack de observabilidade (como kube-prometheus-stack ou Prometheus Operator) para coletar m√©tricas tanto da infraestrutura quanto da API. O Ingress Controller j√° oferece um ponto de entrada HTTP que pode ser reutilizado para expor dashboards ou endpoints de m√©tricas.</p></li><li><p><strong>TLS e dom√≠nio customizado (HTTPS):</strong>
Como a API j√° est√° exposta por um Elastic IP, √© simples criar um registro DNS em um dom√≠nio pr√≥prio (via Route 53 ou outro provedor) apontando para esse IP. Em seguida, basta adicionar um novo Ingress com host definido (por exemplo, <code>api.metrics.meudominio.com</code>) e integrar um emissor de certificados (como AWS Certificate Manager ou Let&rsquo;s Encrypt) para servir tr√°fego HTTPS.</p></li><li><p><strong>Amplia√ß√£o do cluster e novos servi√ßos:</strong>
A inst√¢ncia atual pode ser o ponto de partida para hospedar outros microservices relacionados a m√©tricas, dashboards ou APIs auxiliares. Novos namespaces, Deployments e Services podem ser adicionados ao cluster, todos expostos via NGINX Ingress Controller com regras espec√≠ficas de rota e host.</p></li></ul><p>Em resumo, a inst√¢ncia EC2 ‚Äî combinada com k3s, Ingress NGINX, Docker e a pipeline CI/CD no GitHub Actions ‚Äî passa a operar como um ambiente de aplica√ß√£o completo, apto n√£o apenas para servir a API de m√©tricas em produ√ß√£o, mas tamb√©m para suportar testes, experimentos e futuras extens√µes em termos de observabilidade, seguran√ßa (TLS) e escalabilidade l√≥gica da solu√ß√£o.</p></div></div><aside class="toc toc-sticky"><p class=toc-title><strong>Sum√°rio</strong></p><div class=toc-inner><nav id=TableOfContents><ul><li><a href=#propr√≥sito-deste-projeto>Propr√≥sito deste projeto</a><ul><li><a href=#objetivos-deste-projeto>Objetivos deste projeto</a></li><li><a href=#diagrama-de-arquitetura-do-projeto>Diagrama de arquitetura do projeto</a></li></ul></li><li><a href=#api-com-fastapi>API com FastAPI</a></li><li><a href=#conteineriza√ß√£o-da-api-com-docker>Conteineriza√ß√£o da API com Docker</a><ul><li><a href=#arquivo-docker-na-raiz-do-projeto>Arquivo Docker na raiz do projeto</a></li><li><a href=#build-e-push-manual-da-imagem-para-teste>Build e push manual da imagem (para teste)</a></li></ul></li><li><a href=#orquestra√ß√£o-em-um-cluster-kubernetes>Orquestra√ß√£o em um cluster Kubernetes</a></li><li><a href=#provisionando-a-infraestrutura-via-terraform>Provisionando a infraestrutura via Terraform</a><ul><li><a href=#configurando-a-vpc>Configurando a VPC</a></li><li><a href=#configurando-um-security-group>Configurando um Security Group</a></li><li><a href=#configurando-a-inst√¢ncia-ec2>Configurando a inst√¢ncia EC2</a></li></ul></li><li><a href=#implementando-as-configura√ß√µes-com-ansible>Implementando as configura√ß√µes com Ansible</a></li><li><a href=#pipeline-cicd-com-github-actions>Pipeline CI/CD com GitHub Actions</a><ul><li><a href=#bootstrap-do-cluster-na-inst√¢ncia-ec2>Bootstrap do cluster na inst√¢ncia EC2</a></li><li><a href=#testes-realizados-dentro-da-inst√¢ncia-ec2-ap√≥s-deploy>Testes realizados dentro da inst√¢ncia EC2 ap√≥s deploy</a></li></ul></li><li><a href=#sum√°rio-acesso-√†-api-na-ec2-e-utiliza√ß√£o-pr√°tica-da-inst√¢ncia>Sum√°rio: Acesso √† API na EC2 e utiliza√ß√£o pr√°tica da inst√¢ncia</a><ul><li><a href=#caracter√≠sticas-da-inst√¢ncia-e-do-endpoint-p√∫blico>Caracter√≠sticas da inst√¢ncia e do endpoint p√∫blico</a></li><li><a href=#exposi√ß√£o-da-api-via-ingress-nginx>Exposi√ß√£o da API via Ingress NGINX</a></li><li><a href=#utiliza√ß√£o-da-inst√¢ncia-para-inspe√ß√£o-debug-e-opera√ß√£o>Utiliza√ß√£o da inst√¢ncia para inspe√ß√£o, debug e opera√ß√£o</a></li><li><a href=#extens√µes-futuras-observabilidade-tls-e-dom√≠nios-customizados>Extens√µes futuras: observabilidade, TLS e dom√≠nios customizados</a></li></ul></li></ul></nav></div></aside></div></article><div class=single-pagination><hr><div class=flexnowrap><div class=single-pagination-prev><div class=single-pagination-container-prev><div class=single-pagination-text>‚Üê</div><div class=single-pagination-text><a href=/pt-br/posts/folio-creation/>Folio Cassianico (Arquitetura & Implanta√ß√£o)</a></div></div></div><div class=single-pagination-next></div></div><hr></div><div class=back-to-top><a href=#top>voltar ao topo</a></div></div></main></div><footer><p>Copyright ¬© 2025 C√°ssio Gabriel</p></footer></body><script src=/js/theme-switch.js></script><script defer src=/js/copy-code.js></script></html>